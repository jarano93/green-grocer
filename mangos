#!/bin/zsh


# Set output directory and config file
readonly OUTPUT_DIR="$HOME/fresh_mangos"
readonly CONFIG_FILE="$HOME/.config/mangos.config"
# Get values from config file
readonly SLEEP_TIME=$(grep "sleep" $CONFIG_FILE | grep -Po "=.+" | grep -Po '\w+')
readonly MEMBER_ID=$(grep "member_id" $CONFIG_FILE | grep -Po "=.+" | grep -Po "\w+")
readonly PASS_HASH=$(grep "pass_hash" $CONFIG_FILE | grep -Po "=.+" | grep -Po "\w+")
# get absoulte paths to local directories
readonly DIR=$(dirname $0)
readonly PY="$DIR/py2"
readonly JS="$DIR/js"
readonly ZSH="$DIR/zsh"
readonly PL="$DIR/pl"

#################
##  FUNCTIONS  ##
#################

# creates a dirctory in $OUTPUT_DIR, deletes it if it already exists
clean_dir () {
    if [ -d "$OUTPUT_DIR/$1" ]
    then
        rm -rf "$OUTPUT_DIR/$1"
    fi
    mkdir "$OUTPUT_DIR/$1"
}

# creates a directory in $OUTPUT_DIR/$1, deletes it if it already exists
clean_subdir () {
    if [ -d "$OUTPUT_DIR/$1/$2" ]
    then
        rm -rf "$OUTPUT_DIR/$1/$2"
    fi
    mkdir "$OUTPUT_DIR/$1/$2"
}

# gets all the urls of every page in a single chapter
get_page_urls () {
    page_html=$(phantomjs $JS/page_html.js $MEMBER_ID $PASS_HASH $1)
    echo $(python2 $PY/get_page_urls.py $(echo $page_html) | grep -Po '<option [^\n]+>' | grep -Po 'http://bato.to/reader#\w+')
}

# returns a single line by index in a multi-line string
line () {
    if (( $2 == 1 ))
    then
        echo $(echo $1 | head -n 1)
    else
        echo $(echo $1 | head -n $2 | tail -n 1)
    fi
}

# populates a directory (i.e. downloads all the images)
populate_subdir () {
    page_urls=($(get_page_urls $1))
    num_pages=$(echo $page_urls | wc -w)
    echo -n "    preparing to download $num_pages pages"
    page=1
    for page_url in $page_urls
    do
        sleep $SLEEP_TIME
        echo -n "\r    downloading page $page of $num_pages pages"
        zsh $ZSH/get_image.zsh $MEMBER_ID $PASS_HASH $page_url "$OUTPUT_DIR/$2/$3"
        (( page++ ))
    done
    echo -n "\r"
}


# creates $OUTPUT_DIR if it does not exist
if [ ! -d "$OUTPUT_DIR" ]
then
    mkdir $OUTPUT_DIR
fi

echo "connecting to: $1"

# get series page HTML
SERIES_HTML=$(phantomjs $JS/series_html.js $MEMBER_ID $PASS_HASH $1)
# scrape series page HTML with python
series_name=$(python2 $PY/get_series_name.py $(echo $SERIES_HTML))
chapter_urls=$(python2 $PY/get_chapter_urls.py $(echo $SERIES_HTML))
# sleep so bato.to servers don't notice you hitting them up to fast 
sleep $SLEEP_TIME
# get english row HTML because python can't handle unicode, fam
eng_row=$(phantomjs $JS/eng_row_html.js $MEMBER_ID $PASS_HASH $1)
# grep chapter names and group names from the english row HTML
chapter_names=$(zsh $ZSH/get_chapter_names.zsh $eng_row)
chapter_groups=$(zsh $ZSH/get_chapter_groups.zsh $eng_row)

echo "Series: $series_name"
length=$(echo $chapter_urls | wc -l)

series_dir="$series_name"
clean_dir $series_dir

i=$length
j=0
# urls, names, groups are in descending order, so loop from the bottom up to get
# chapters in ascending order
while (( $i > 0 ))
do
    sleep $SLEEP_TIME
    ch_name=$(line $chapter_names $i)
    ch_group=$(line $chapter_groups $i)
    # replace '/' with '-' so mkdir can handle them
    rep_name=$(perl $PL/replace_forwardslash.pl "$(echo $ch_name)")
    rep_group=$(perl $PL/replace_forwardslash.pl "$(echo $ch_group)")
    ch_url=$(line $chapter_urls $i)
    ch_dir=$(echo "[$rep_group] $rep_name")
    clean_subdir $series_dir $ch_dir
    # display progress
    zsh $ZSH/progress.zsh $(echo "Downloading: $ch_dir --end_status_string $j $length")
    populate_subdir $ch_url $series_dir $ch_dir
    (( i-- ))
    (( j++ ))
done
zsh $ZSH/progress.zsh  $(echo "Downloaded: $series_name! --end_status_string $length $length")
